{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "DATA_FILE       = '../data/nasa.p'\n",
        "PREFIX          = 'nasa/nasa_d2v_'\n",
        "\n",
        "def extract_title(datum):\n",
        "    return datum['Collection']['ShortName'] + ' ' + datum['Collection']['LongName']"
      ],
      "outputs": [],
      "execution_count": 2,
      "metadata": {
        "collapsed": false,
        "outputHidden": false,
        "inputHidden": false
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import sys\n",
        "module_path = os.path.abspath(os.path.join('..'))\n",
        "if module_path not in sys.path:\n",
        "    sys.path.append(module_path)"
      ],
      "outputs": [],
      "execution_count": 3,
      "metadata": {
        "collapsed": false,
        "outputHidden": false,
        "inputHidden": false
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import time\n",
        "from datetime import datetime as dt\n",
        "import json\n",
        "import pickle\n",
        "import random\n",
        "from os.path import join\n",
        "from pathlib import Path\n",
        "import logging\n",
        "\n",
        "from cleaning.serialize import struct2sentence\n",
        "import stdlog\n",
        "from gensim.models.doc2vec import Doc2Vec\n",
        "from gensim.models.doc2vec import TaggedDocument"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "2018-05-08 16:47:35,445 : INFO : 'pattern' package not found; tag filters are not available for English\n"
          ]
        }
      ],
      "execution_count": 4,
      "metadata": {
        "collapsed": false,
        "outputHidden": false,
        "inputHidden": false
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load pickled dataset in entirety\n",
        "long_names, metadata = pickle.load(open(DATA_FILE, 'rb'))"
      ],
      "outputs": [],
      "execution_count": 5,
      "metadata": {
        "collapsed": false,
        "outputHidden": false,
        "inputHidden": false
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import multiprocessing\n",
        "\n",
        "# Parallelize serialization of data into sentences\n",
        "pool = multiprocessing.Pool()\n",
        "sentences_2d = pool.map(struct2sentence, metadata)"
      ],
      "outputs": [],
      "execution_count": 6,
      "metadata": {
        "collapsed": false,
        "outputHidden": false,
        "inputHidden": false
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# We need to feed it labeled sentences\n",
        "doc_sentences = []\n",
        "for idx, sentence_list in enumerate(sentences_2d):\n",
        "    \n",
        "    # This is dataset specific\n",
        "    ln = extract_title(metadata[idx])\n",
        "    \n",
        "    for sentence in sentence_list:\n",
        "        words = list(filter(None, sentence.split(' ')))\n",
        "        ls = TaggedDocument(words=words, tags=[str(idx), ln])\n",
        "        doc_sentences.append(ls)"
      ],
      "outputs": [],
      "execution_count": 7,
      "metadata": {
        "collapsed": false,
        "outputHidden": false,
        "inputHidden": false
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create and train a new model\n",
        "model = Doc2Vec(doc_sentences, size=100, window=8, min_count=5, workers=7)"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/Users/gabemontague/Courses/CS91R/earth-speaks/env/lib/python3.6/site-packages/gensim/models/doc2vec.py:366: UserWarning: The parameter `size` is deprecated, will be removed in 4.0.0, use `vector_size` instead.\n",
            "  warnings.warn(\"The parameter `size` is deprecated, will be removed in 4.0.0, use `vector_size` instead.\")\n",
            "2018-05-08 16:35:17,954 : INFO : collecting all words and their counts\n",
            "2018-05-08 16:35:17,954 : INFO : PROGRESS: at example #0, processed 0 words (0/s), 0 word types, 0 tags\n",
            "2018-05-08 16:35:18,041 : INFO : PROGRESS: at example #10000, processed 81182 words (943698/s), 4421 word types, 146 tags\n",
            "2018-05-08 16:35:18,124 : INFO : PROGRESS: at example #20000, processed 167233 words (1052366/s), 6483 word types, 245 tags\n",
            "2018-05-08 16:35:18,216 : INFO : PROGRESS: at example #30000, processed 246946 words (873045/s), 8344 word types, 305 tags\n",
            "2018-05-08 16:35:18,308 : INFO : PROGRESS: at example #40000, processed 336871 words (985680/s), 8925 word types, 425 tags\n",
            "2018-05-08 16:35:18,394 : INFO : PROGRESS: at example #50000, processed 417503 words (950176/s), 10728 word types, 583 tags\n",
            "2018-05-08 16:35:18,485 : INFO : PROGRESS: at example #60000, processed 509774 words (1024970/s), 12750 word types, 722 tags\n",
            "2018-05-08 16:35:18,577 : INFO : PROGRESS: at example #70000, processed 600283 words (996079/s), 14041 word types, 908 tags\n",
            "2018-05-08 16:35:18,672 : INFO : PROGRESS: at example #80000, processed 697393 words (1035226/s), 16266 word types, 1081 tags\n",
            "2018-05-08 16:35:18,769 : INFO : PROGRESS: at example #90000, processed 788304 words (942551/s), 17271 word types, 1262 tags\n",
            "2018-05-08 16:35:18,860 : INFO : PROGRESS: at example #100000, processed 875557 words (974255/s), 18314 word types, 1457 tags\n",
            "2018-05-08 16:35:18,955 : INFO : PROGRESS: at example #110000, processed 964214 words (941954/s), 19561 word types, 1581 tags\n",
            "2018-05-08 16:35:19,053 : INFO : PROGRESS: at example #120000, processed 1061305 words (998472/s), 21202 word types, 1749 tags\n",
            "2018-05-08 16:35:19,145 : INFO : PROGRESS: at example #130000, processed 1158159 words (1068202/s), 22675 word types, 1915 tags\n",
            "2018-05-08 16:35:19,240 : INFO : PROGRESS: at example #140000, processed 1251007 words (978369/s), 23534 word types, 2076 tags\n",
            "2018-05-08 16:35:19,331 : INFO : PROGRESS: at example #150000, processed 1336319 words (949744/s), 24218 word types, 2222 tags\n",
            "2018-05-08 16:35:19,424 : INFO : PROGRESS: at example #160000, processed 1421938 words (934124/s), 24836 word types, 2406 tags\n",
            "2018-05-08 16:35:19,514 : INFO : PROGRESS: at example #170000, processed 1513279 words (1025349/s), 25353 word types, 2517 tags\n",
            "2018-05-08 16:35:19,607 : INFO : PROGRESS: at example #180000, processed 1596153 words (902551/s), 26327 word types, 2597 tags\n",
            "2018-05-08 16:35:19,703 : INFO : PROGRESS: at example #190000, processed 1675668 words (834151/s), 27340 word types, 2750 tags\n",
            "2018-05-08 16:35:19,793 : INFO : PROGRESS: at example #200000, processed 1757311 words (916580/s), 27992 word types, 2843 tags\n",
            "2018-05-08 16:35:19,884 : INFO : PROGRESS: at example #210000, processed 1842434 words (946426/s), 28510 word types, 2921 tags\n",
            "2018-05-08 16:35:19,977 : INFO : PROGRESS: at example #220000, processed 1927947 words (929807/s), 29460 word types, 3023 tags\n",
            "2018-05-08 16:35:20,070 : INFO : PROGRESS: at example #230000, processed 2014418 words (946979/s), 31157 word types, 3151 tags\n",
            "2018-05-08 16:35:20,163 : INFO : PROGRESS: at example #240000, processed 2098811 words (917755/s), 32121 word types, 3353 tags\n",
            "2018-05-08 16:35:20,197 : INFO : collected 32472 word types and 3430 unique tags from a corpus of 243409 examples and 2130005 words\n",
            "2018-05-08 16:35:20,198 : INFO : Loading a fresh vocabulary\n",
            "2018-05-08 16:35:20,230 : INFO : min_count=5 retains 8955 unique words (27% of original 32472, drops 23517)\n",
            "2018-05-08 16:35:20,231 : INFO : min_count=5 leaves 2094187 word corpus (98% of original 2130005, drops 35818)\n",
            "2018-05-08 16:35:20,256 : INFO : deleting the raw counts dictionary of 32472 items\n",
            "2018-05-08 16:35:20,258 : INFO : sample=0.001 downsamples 58 most-common words\n",
            "2018-05-08 16:35:20,259 : INFO : downsampling leaves estimated 1422456 word corpus (67.9% of prior 2094187)\n",
            "2018-05-08 16:35:20,281 : INFO : estimated required memory for 8955 words and 100 dimensions: 13699500 bytes\n",
            "2018-05-08 16:35:20,283 : INFO : resetting layer weights\n",
            "2018-05-08 16:35:20,427 : INFO : training model with 7 workers on 8955 vocabulary and 100 features, using sg=0 hs=0 sample=0.001 negative=5 window=8\n",
            "2018-05-08 16:35:21,494 : INFO : EPOCH 1 - PROGRESS: at 4.02% examples, 65044 words/s, in_qsize 13, out_qsize 0\n",
            "2018-05-08 16:35:22,510 : INFO : EPOCH 1 - PROGRESS: at 10.68% examples, 90795 words/s, in_qsize 13, out_qsize 0\n",
            "2018-05-08 16:35:23,758 : INFO : EPOCH 1 - PROGRESS: at 17.41% examples, 94112 words/s, in_qsize 13, out_qsize 0\n",
            "2018-05-08 16:35:24,809 : INFO : EPOCH 1 - PROGRESS: at 23.73% examples, 98313 words/s, in_qsize 13, out_qsize 0\n",
            "2018-05-08 16:35:25,963 : INFO : EPOCH 1 - PROGRESS: at 29.76% examples, 100546 words/s, in_qsize 13, out_qsize 0\n",
            "2018-05-08 16:35:26,996 : INFO : EPOCH 1 - PROGRESS: at 35.81% examples, 103850 words/s, in_qsize 13, out_qsize 0\n",
            "2018-05-08 16:35:27,998 : INFO : EPOCH 1 - PROGRESS: at 42.07% examples, 105652 words/s, in_qsize 14, out_qsize 0\n",
            "2018-05-08 16:35:29,099 : INFO : EPOCH 1 - PROGRESS: at 48.65% examples, 107413 words/s, in_qsize 14, out_qsize 0\n",
            "2018-05-08 16:35:30,105 : INFO : EPOCH 1 - PROGRESS: at 54.95% examples, 110195 words/s, in_qsize 14, out_qsize 0\n",
            "2018-05-08 16:35:31,128 : INFO : EPOCH 1 - PROGRESS: at 60.61% examples, 109662 words/s, in_qsize 13, out_qsize 0\n",
            "2018-05-08 16:35:32,137 : INFO : EPOCH 1 - PROGRESS: at 66.75% examples, 110415 words/s, in_qsize 13, out_qsize 0\n",
            "2018-05-08 16:35:33,192 : INFO : EPOCH 1 - PROGRESS: at 72.11% examples, 109821 words/s, in_qsize 13, out_qsize 0\n",
            "2018-05-08 16:35:34,289 : INFO : EPOCH 1 - PROGRESS: at 79.43% examples, 110025 words/s, in_qsize 14, out_qsize 0\n",
            "2018-05-08 16:35:35,415 : INFO : EPOCH 1 - PROGRESS: at 86.30% examples, 110212 words/s, in_qsize 13, out_qsize 0\n",
            "2018-05-08 16:35:36,427 : INFO : EPOCH 1 - PROGRESS: at 92.55% examples, 110594 words/s, in_qsize 13, out_qsize 0\n",
            "2018-05-08 16:35:37,393 : INFO : worker thread finished; awaiting finish of 6 more threads\n",
            "2018-05-08 16:35:37,405 : INFO : worker thread finished; awaiting finish of 5 more threads\n",
            "2018-05-08 16:35:37,430 : INFO : EPOCH 1 - PROGRESS: at 98.24% examples, 110342 words/s, in_qsize 4, out_qsize 1\n",
            "2018-05-08 16:35:37,432 : INFO : worker thread finished; awaiting finish of 4 more threads\n",
            "2018-05-08 16:35:37,459 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
            "2018-05-08 16:35:37,468 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
            "2018-05-08 16:35:37,485 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
            "2018-05-08 16:35:37,496 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
            "2018-05-08 16:35:37,497 : INFO : EPOCH - 1 : training on 2130005 raw words (1909254 effective words) took 17.1s, 111918 effective words/s\n",
            "2018-05-08 16:35:38,582 : INFO : EPOCH 2 - PROGRESS: at 4.02% examples, 64190 words/s, in_qsize 13, out_qsize 0\n",
            "2018-05-08 16:35:39,648 : INFO : EPOCH 2 - PROGRESS: at 10.68% examples, 88005 words/s, in_qsize 14, out_qsize 0\n",
            "2018-05-08 16:35:40,799 : INFO : EPOCH 2 - PROGRESS: at 17.41% examples, 95025 words/s, in_qsize 13, out_qsize 0\n",
            "2018-05-08 16:35:41,812 : INFO : EPOCH 2 - PROGRESS: at 24.14% examples, 102063 words/s, in_qsize 14, out_qsize 0\n",
            "2018-05-08 16:35:42,864 : INFO : EPOCH 2 - PROGRESS: at 29.85% examples, 104001 words/s, in_qsize 14, out_qsize 0\n",
            "2018-05-08 16:35:43,952 : INFO : EPOCH 2 - PROGRESS: at 35.81% examples, 105852 words/s, in_qsize 14, out_qsize 0\n",
            "2018-05-08 16:35:44,978 : INFO : EPOCH 2 - PROGRESS: at 42.07% examples, 107086 words/s, in_qsize 13, out_qsize 0\n",
            "2018-05-08 16:35:46,129 : INFO : EPOCH 2 - PROGRESS: at 48.65% examples, 108072 words/s, in_qsize 14, out_qsize 0\n",
            "2018-05-08 16:35:47,177 : INFO : EPOCH 2 - PROGRESS: at 54.95% examples, 110310 words/s, in_qsize 13, out_qsize 0\n",
            "2018-05-08 16:35:48,198 : INFO : EPOCH 2 - PROGRESS: at 60.61% examples, 109816 words/s, in_qsize 13, out_qsize 0\n",
            "2018-05-08 16:35:49,222 : INFO : EPOCH 2 - PROGRESS: at 65.84% examples, 108823 words/s, in_qsize 13, out_qsize 0\n",
            "2018-05-08 16:35:50,389 : INFO : EPOCH 2 - PROGRESS: at 72.11% examples, 108866 words/s, in_qsize 13, out_qsize 0\n",
            "2018-05-08 16:35:51,477 : INFO : EPOCH 2 - PROGRESS: at 79.43% examples, 109213 words/s, in_qsize 14, out_qsize 0\n",
            "2018-05-08 16:35:52,513 : INFO : EPOCH 2 - PROGRESS: at 85.83% examples, 109517 words/s, in_qsize 14, out_qsize 0\n",
            "2018-05-08 16:35:53,560 : INFO : EPOCH 2 - PROGRESS: at 91.62% examples, 109165 words/s, in_qsize 14, out_qsize 0\n",
            "2018-05-08 16:35:54,529 : INFO : worker thread finished; awaiting finish of 6 more threads\n",
            "2018-05-08 16:35:54,538 : INFO : worker thread finished; awaiting finish of 5 more threads\n",
            "2018-05-08 16:35:54,585 : INFO : EPOCH 2 - PROGRESS: at 98.24% examples, 109881 words/s, in_qsize 4, out_qsize 1\n",
            "2018-05-08 16:35:54,587 : INFO : worker thread finished; awaiting finish of 4 more threads\n",
            "2018-05-08 16:35:54,611 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
            "2018-05-08 16:35:54,633 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
            "2018-05-08 16:35:54,655 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
            "2018-05-08 16:35:54,662 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
            "2018-05-08 16:35:54,663 : INFO : EPOCH - 2 : training on 2130005 raw words (1910313 effective words) took 17.2s, 111371 effective words/s\n",
            "2018-05-08 16:35:55,729 : INFO : EPOCH 3 - PROGRESS: at 4.02% examples, 65494 words/s, in_qsize 13, out_qsize 0\n",
            "2018-05-08 16:35:56,751 : INFO : EPOCH 3 - PROGRESS: at 10.68% examples, 90637 words/s, in_qsize 13, out_qsize 0\n",
            "2018-05-08 16:35:57,881 : INFO : EPOCH 3 - PROGRESS: at 17.41% examples, 97557 words/s, in_qsize 14, out_qsize 0\n",
            "2018-05-08 16:35:58,901 : INFO : EPOCH 3 - PROGRESS: at 24.55% examples, 105987 words/s, in_qsize 13, out_qsize 0\n",
            "2018-05-08 16:36:00,011 : INFO : EPOCH 3 - PROGRESS: at 30.71% examples, 107386 words/s, in_qsize 13, out_qsize 0\n",
            "2018-05-08 16:36:01,050 : INFO : EPOCH 3 - PROGRESS: at 36.31% examples, 108354 words/s, in_qsize 13, out_qsize 0\n",
            "2018-05-08 16:36:02,057 : INFO : EPOCH 3 - PROGRESS: at 42.93% examples, 110650 words/s, in_qsize 13, out_qsize 0\n",
            "2018-05-08 16:36:03,134 : INFO : EPOCH 3 - PROGRESS: at 48.65% examples, 110030 words/s, in_qsize 14, out_qsize 0\n",
            "2018-05-08 16:36:04,173 : INFO : EPOCH 3 - PROGRESS: at 54.95% examples, 112193 words/s, in_qsize 14, out_qsize 0\n",
            "2018-05-08 16:36:05,223 : INFO : EPOCH 3 - PROGRESS: at 60.61% examples, 111190 words/s, in_qsize 13, out_qsize 0\n",
            "2018-05-08 16:36:06,224 : INFO : EPOCH 3 - PROGRESS: at 66.75% examples, 111905 words/s, in_qsize 13, out_qsize 0\n",
            "2018-05-08 16:36:07,228 : INFO : EPOCH 3 - PROGRESS: at 72.11% examples, 111627 words/s, in_qsize 13, out_qsize 0\n",
            "2018-05-08 16:36:08,298 : INFO : EPOCH 3 - PROGRESS: at 79.43% examples, 111916 words/s, in_qsize 14, out_qsize 0\n",
            "2018-05-08 16:36:09,418 : INFO : EPOCH 3 - PROGRESS: at 86.27% examples, 112037 words/s, in_qsize 13, out_qsize 0\n",
            "2018-05-08 16:36:10,442 : INFO : EPOCH 3 - PROGRESS: at 93.01% examples, 112735 words/s, in_qsize 14, out_qsize 0\n",
            "2018-05-08 16:36:11,193 : INFO : worker thread finished; awaiting finish of 6 more threads\n",
            "2018-05-08 16:36:11,214 : INFO : worker thread finished; awaiting finish of 5 more threads\n",
            "2018-05-08 16:36:11,221 : INFO : worker thread finished; awaiting finish of 4 more threads\n",
            "2018-05-08 16:36:11,269 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
            "2018-05-08 16:36:11,282 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
            "2018-05-08 16:36:11,309 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
            "2018-05-08 16:36:11,316 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
            "2018-05-08 16:36:11,317 : INFO : EPOCH - 3 : training on 2130005 raw words (1909585 effective words) took 16.6s, 114758 effective words/s\n",
            "2018-05-08 16:36:12,352 : INFO : EPOCH 4 - PROGRESS: at 4.02% examples, 67081 words/s, in_qsize 13, out_qsize 0\n",
            "2018-05-08 16:36:13,389 : INFO : EPOCH 4 - PROGRESS: at 10.68% examples, 91234 words/s, in_qsize 13, out_qsize 0\n",
            "2018-05-08 16:36:14,520 : INFO : EPOCH 4 - PROGRESS: at 17.41% examples, 97874 words/s, in_qsize 13, out_qsize 0\n",
            "2018-05-08 16:36:15,522 : INFO : EPOCH 4 - PROGRESS: at 24.14% examples, 104582 words/s, in_qsize 14, out_qsize 0\n",
            "2018-05-08 16:36:16,645 : INFO : EPOCH 4 - PROGRESS: at 30.71% examples, 107719 words/s, in_qsize 13, out_qsize 0\n",
            "2018-05-08 16:36:17,650 : INFO : EPOCH 4 - PROGRESS: at 36.72% examples, 110665 words/s, in_qsize 13, out_qsize 0\n",
            "2018-05-08 16:36:18,669 : INFO : EPOCH 4 - PROGRESS: at 43.34% examples, 112373 words/s, in_qsize 13, out_qsize 0\n",
            "2018-05-08 16:36:19,706 : INFO : EPOCH 4 - PROGRESS: at 49.53% examples, 113267 words/s, in_qsize 13, out_qsize 0\n",
            "2018-05-08 16:36:20,759 : INFO : EPOCH 4 - PROGRESS: at 55.86% examples, 114919 words/s, in_qsize 13, out_qsize 0\n",
            "2018-05-08 16:36:21,934 : INFO : EPOCH 4 - PROGRESS: at 62.55% examples, 114045 words/s, in_qsize 14, out_qsize 0\n",
            "2018-05-08 16:36:23,025 : INFO : EPOCH 4 - PROGRESS: at 69.08% examples, 114308 words/s, in_qsize 13, out_qsize 0\n",
            "2018-05-08 16:36:24,146 : INFO : EPOCH 4 - PROGRESS: at 75.82% examples, 114017 words/s, in_qsize 13, out_qsize 0\n",
            "2018-05-08 16:36:25,155 : INFO : EPOCH 4 - PROGRESS: at 81.40% examples, 112774 words/s, in_qsize 14, out_qsize 0\n",
            "2018-05-08 16:36:26,175 : INFO : EPOCH 4 - PROGRESS: at 87.72% examples, 113051 words/s, in_qsize 13, out_qsize 0\n",
            "2018-05-08 16:36:27,186 : INFO : EPOCH 4 - PROGRESS: at 93.42% examples, 112578 words/s, in_qsize 14, out_qsize 0\n",
            "2018-05-08 16:36:27,877 : INFO : worker thread finished; awaiting finish of 6 more threads\n",
            "2018-05-08 16:36:27,885 : INFO : worker thread finished; awaiting finish of 5 more threads\n",
            "2018-05-08 16:36:27,918 : INFO : worker thread finished; awaiting finish of 4 more threads\n",
            "2018-05-08 16:36:27,951 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
            "2018-05-08 16:36:27,969 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
            "2018-05-08 16:36:27,987 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
            "2018-05-08 16:36:27,996 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
            "2018-05-08 16:36:27,997 : INFO : EPOCH - 4 : training on 2130005 raw words (1909563 effective words) took 16.7s, 114539 effective words/s\n",
            "2018-05-08 16:36:29,054 : INFO : EPOCH 5 - PROGRESS: at 4.02% examples, 66826 words/s, in_qsize 14, out_qsize 0\n",
            "2018-05-08 16:36:30,123 : INFO : EPOCH 5 - PROGRESS: at 10.68% examples, 89615 words/s, in_qsize 14, out_qsize 0\n",
            "2018-05-08 16:36:31,126 : INFO : EPOCH 5 - PROGRESS: at 16.90% examples, 97898 words/s, in_qsize 13, out_qsize 0\n",
            "2018-05-08 16:36:32,226 : INFO : EPOCH 5 - PROGRESS: at 21.88% examples, 93729 words/s, in_qsize 13, out_qsize 0\n",
            "2018-05-08 16:36:33,370 : INFO : EPOCH 5 - PROGRESS: at 28.16% examples, 97471 words/s, in_qsize 13, out_qsize 0\n",
            "2018-05-08 16:36:34,441 : INFO : EPOCH 5 - PROGRESS: at 33.72% examples, 99148 words/s, in_qsize 14, out_qsize 0\n",
            "2018-05-08 16:36:35,447 : INFO : EPOCH 5 - PROGRESS: at 39.22% examples, 100451 words/s, in_qsize 14, out_qsize 0\n",
            "2018-05-08 16:36:36,464 : INFO : EPOCH 5 - PROGRESS: at 45.28% examples, 101964 words/s, in_qsize 13, out_qsize 0\n",
            "2018-05-08 16:36:37,490 : INFO : EPOCH 5 - PROGRESS: at 51.53% examples, 104941 words/s, in_qsize 13, out_qsize 0\n",
            "2018-05-08 16:36:38,522 : INFO : EPOCH 5 - PROGRESS: at 57.70% examples, 106632 words/s, in_qsize 13, out_qsize 0\n",
            "2018-05-08 16:36:39,522 : INFO : EPOCH 5 - PROGRESS: at 63.48% examples, 106816 words/s, in_qsize 13, out_qsize 0\n",
            "2018-05-08 16:36:40,526 : INFO : EPOCH 5 - PROGRESS: at 69.87% examples, 108366 words/s, in_qsize 13, out_qsize 0\n",
            "2018-05-08 16:36:41,583 : INFO : EPOCH 5 - PROGRESS: at 75.82% examples, 107774 words/s, in_qsize 14, out_qsize 0\n",
            "2018-05-08 16:36:42,622 : INFO : EPOCH 5 - PROGRESS: at 82.46% examples, 108052 words/s, in_qsize 14, out_qsize 0\n",
            "2018-05-08 16:36:43,636 : INFO : EPOCH 5 - PROGRESS: at 88.71% examples, 108697 words/s, in_qsize 13, out_qsize 0\n",
            "2018-05-08 16:36:44,708 : INFO : EPOCH 5 - PROGRESS: at 94.84% examples, 108525 words/s, in_qsize 11, out_qsize 0\n",
            "2018-05-08 16:36:45,086 : INFO : worker thread finished; awaiting finish of 6 more threads\n",
            "2018-05-08 16:36:45,103 : INFO : worker thread finished; awaiting finish of 5 more threads\n",
            "2018-05-08 16:36:45,143 : INFO : worker thread finished; awaiting finish of 4 more threads\n",
            "2018-05-08 16:36:45,175 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
            "2018-05-08 16:36:45,187 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
            "2018-05-08 16:36:45,203 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
            "2018-05-08 16:36:45,215 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
            "2018-05-08 16:36:45,216 : INFO : EPOCH - 5 : training on 2130005 raw words (1908639 effective words) took 17.2s, 111012 effective words/s\n",
            "2018-05-08 16:36:45,216 : INFO : training on a 10650025 raw words (9547354 effective words) took 84.8s, 112598 effective words/s\n"
          ]
        }
      ],
      "execution_count": 19,
      "metadata": {
        "collapsed": false,
        "outputHidden": false,
        "inputHidden": false
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#model.save(PREFIX + 'basic.m')\n",
        "model = Doc2Vec.load(PREFIX + 'basic.m')"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "2018-05-08 16:47:52,510 : INFO : loading Doc2Vec object from nasa/nasa_d2v_basic.m\n",
            "2018-05-08 16:47:52,597 : INFO : loading vocabulary recursively from nasa/nasa_d2v_basic.m.vocabulary.* with mmap=None\n",
            "2018-05-08 16:47:52,598 : INFO : loading trainables recursively from nasa/nasa_d2v_basic.m.trainables.* with mmap=None\n",
            "2018-05-08 16:47:52,599 : INFO : loading wv recursively from nasa/nasa_d2v_basic.m.wv.* with mmap=None\n",
            "2018-05-08 16:47:52,599 : INFO : loading docvecs recursively from nasa/nasa_d2v_basic.m.docvecs.* with mmap=None\n",
            "2018-05-08 16:47:52,600 : INFO : loaded nasa/nasa_d2v_basic.m\n"
          ]
        }
      ],
      "execution_count": 8,
      "metadata": {
        "collapsed": false,
        "outputHidden": false,
        "inputHidden": false
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Investigation of model performance"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "# Now lets see which is the most similiar to a chosen document\n",
        "model.docvecs.most_similar(300) "
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "2018-05-08 16:47:56,045 : INFO : precomputing L2-norms of doc weight vectors\n"
          ]
        },
        {
          "output_type": "execute_result",
          "execution_count": 9,
          "data": {
            "text/plain": [
              "[('NACP VPRM NEE Parameters Optimized to North American Flux Tower Sites, 2000-2006',\n",
              "  0.9999034404754639),\n",
              " ('MODIS NDVI Data, Smoothed and Gap-filled, for the Conterminous US: 2000-2015',\n",
              "  0.929338276386261),\n",
              " ('224', 0.9284223318099976),\n",
              " ('CARVE: CH4, CO2, and CO Atmospheric Concentrations, CARVE Tower, Alaska, 2012-2014',\n",
              "  0.9039781093597412),\n",
              " ('530', 0.9038670063018799),\n",
              " ('373', 0.9015885591506958),\n",
              " ('Airborne Multi-angle Imaging SpectroRadiometer measurements taken over Wisconsin and the ARM/CART site in Oklahoma. (AIRMISR_WISCONSIN_2000)',\n",
              "  0.9012858867645264),\n",
              " ('948', 0.899218738079071),\n",
              " ('SNF Leaf Optical Properties: Cary-14', 0.8980725407600403),\n",
              " ('FLUXNET Canada Research Network - Canadian Carbon Program Data Collection, 1993-2014',\n",
              "  0.8970130681991577)]"
            ]
          },
          "metadata": {}
        }
      ],
      "execution_count": 9,
      "metadata": {
        "collapsed": false,
        "outputHidden": false,
        "inputHidden": false
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(metadata[300]['Collection']['LongName'])"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "MISR Level 3 Component Global Land Regional public Product covering a day\n"
          ]
        }
      ],
      "execution_count": 10,
      "metadata": {
        "collapsed": false,
        "outputHidden": false,
        "inputHidden": false
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "collapsed": false,
        "outputHidden": false,
        "inputHidden": false
      }
    }
  ],
  "metadata": {
    "kernelspec": {
      "name": "python3",
      "language": "python",
      "display_name": "Python 3"
    },
    "kernel_info": {
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.6.5",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "nteract": {
      "version": "0.8.4"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}